{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DwlU8jBx2Eey"
      },
      "source": [
        "# Assignment 2 Deepankar Varma 102003431 "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Import the required libraries\n",
        "We will need the following libraries to perform face recognition:\n",
        "\n",
        "OpenCV (cv2) for face detection and face alignment. <br>\n",
        "dlib for facial landmarks detection.\n",
        "NumPy for numerical operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "uS9KSTtIzJlX"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import dlib\n",
        "import numpy as np\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Load the dataset into a list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "image_folder = 'Images/Humans'\n",
        "images = []\n",
        "for filename in os.listdir(image_folder):\n",
        "    img = cv2.imread(os.path.join(image_folder, filename))\n",
        "    if img is not None:\n",
        "        images.append(img)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Face detection \n",
        "\n",
        "We will use the Haar Cascade Classifier for face detection. It is a pre-trained classifier that can detect faces in an image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
        "\n",
        "face_rects = []\n",
        "for image in images:\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\n",
        "    for (x,y,w,h) in faces:\n",
        "        face_rects.append((x,y,w,h))\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4: Face Alignment\n",
        "\n",
        "To perform face recognition, we need to align all the faces so that they have similar landmarks. We can use the dlib library to detect facial landmarks and align the faces."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n",
        "\n",
        "aligned_faces = []\n",
        "for image, (x,y,w,h) in zip(images, face_rects):\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    landmarks = predictor(gray, dlib.rectangle(x,y,x+w,y+h))\n",
        "    landmarks = np.array([[p.x, p.y] for p in landmarks.parts()])\n",
        "    \n",
        "    # calculate the mean point of the two corners of the eyes in the image\n",
        "    left_eye = np.mean(landmarks[36:42], axis=0)\n",
        "    right_eye = np.mean(landmarks[42:48], axis=0)\n",
        "\n",
        "    # calculate the angle between the line connecting the eyes and the horizontal axis\n",
        "    dy = right_eye[1] - left_eye[1]\n",
        "    dx = right_eye[0] - left_eye[0]\n",
        "    angle = np.degrees(np.arctan2(dy, dx)) - 180\n",
        "\n",
        "    # calculate the scale factor\n",
        "    desired_left_eye = (0.35, 0.35)\n",
        "    dist = np.sqrt((dx ** 2) + (dy ** 2))\n",
        "    desired_dist = (desired_left_eye[1] - desired_left_eye[0]) * image.shape[1]\n",
        "    scale = desired_dist / dist\n",
        "\n",
        "    # calculate the rotation matrix\n",
        "    center = (left_eye + right_eye) // 2\n",
        "    M = cv2.getRotationMatrix2D(tuple(center), angle, scale)\n",
        "\n",
        "    # apply the transformation to the image\n",
        "    tX = image.shape[1] // 2 - center[0]\n",
        "    tY = image.shape[0] * 0.35 - center[1]\n",
        "    M[0, 2] += tX\n",
        "    M[1, 2] += tY\n",
        "\n",
        "    aligned_face = cv2.warpAffine(image, M, (image.shape[1], image.shape[0]), flags=cv2.INTER_CUBIC)\n",
        "    aligned_faces.append(aligned_face)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5: Feature Extraction\n",
        "\n",
        "To perform feature extraction, we will use a pre-trained deep learning model that can extract facial features. Here, we will use the VGG-Face model, which is a pre-trained CNN model trained on a large dataset of faces. We will use the last fully connected layer (fc7) of the model as the feature vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 313ms/step\n",
            "1/1 [==============================] - 0s 199ms/step\n",
            "1/1 [==============================] - 0s 193ms/step\n",
            "1/1 [==============================] - 0s 208ms/step\n",
            "1/1 [==============================] - 0s 214ms/step\n",
            "1/1 [==============================] - 0s 221ms/step\n",
            "1/1 [==============================] - 0s 211ms/step\n",
            "1/1 [==============================] - 0s 198ms/step\n",
            "1/1 [==============================] - 0s 191ms/step\n",
            "1/1 [==============================] - 0s 205ms/step\n",
            "1/1 [==============================] - 0s 220ms/step\n",
            "1/1 [==============================] - 0s 209ms/step\n",
            "1/1 [==============================] - 0s 228ms/step\n",
            "1/1 [==============================] - 0s 209ms/step\n",
            "1/1 [==============================] - 0s 192ms/step\n",
            "1/1 [==============================] - 0s 193ms/step\n",
            "1/1 [==============================] - 0s 189ms/step\n",
            "1/1 [==============================] - 0s 203ms/step\n",
            "1/1 [==============================] - 0s 243ms/step\n",
            "1/1 [==============================] - 0s 190ms/step\n",
            "1/1 [==============================] - 0s 183ms/step\n",
            "1/1 [==============================] - 0s 191ms/step\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "\n",
        "model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3), pooling='avg')\n",
        "\n",
        "features = []\n",
        "for aligned_face in aligned_faces:\n",
        "    # resize the image to the input size of the VGG16 model\n",
        "    input_image = cv2.resize(aligned_face, (224, 224))\n",
        "\n",
        "    # preprocess the input image\n",
        "    input_image = preprocess_input(input_image)\n",
        "\n",
        "    # extract features using the VGG16 model\n",
        "    feature = model.predict(np.expand_dims(input_image, axis=0))[0]\n",
        "\n",
        "    features.append(feature)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 6: Feature Matching\n",
        "\n",
        "To perform feature matching, we can use any distance metric to calculate the distance between two feature vectors. Here, we will use the Euclidean distance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "import dlib\n",
        "import numpy as np\n",
        "\n",
        "detector = dlib.get_frontal_face_detector()\n",
        "predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n",
        "\n",
        "def align_face(face):\n",
        "    gray = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
        "    rects = detector(gray, 1)\n",
        "    if len(rects) == 0:\n",
        "        return None\n",
        "    shape = predictor(gray, rects[0])\n",
        "    landmarks = np.array([[p.x, p.y] for p in shape.parts()])\n",
        "    eye_center = landmarks[36:42].mean(axis=0)\n",
        "    dx = landmarks[45, 0] - landmarks[36, 0]\n",
        "    dy = landmarks[45, 1] - landmarks[36, 1]\n",
        "    angle = np.degrees(np.arctan2(dy, dx)) - 90\n",
        "    M = cv2.getRotationMatrix2D(tuple(eye_center), angle, 1)\n",
        "    aligned_face = cv2.warpAffine(face, M, (face.shape[1], face.shape[0]), flags=cv2.INTER_CUBIC)\n",
        "    return aligned_face\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 178ms/step\n",
            "No match found\n"
          ]
        }
      ],
      "source": [
        "from scipy.spatial.distance import euclidean\n",
        "\n",
        "def distance(feature1, feature2):\n",
        "    return euclidean(feature1, feature2)\n",
        "\n",
        "def match(feature, database, threshold):\n",
        "    distances = [distance(feature, db_feature) for db_feature in database]\n",
        "    min_distance = min(distances)\n",
        "    if min_distance < threshold:\n",
        "        return np.argmin(distances)\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "database = np.array(features)\n",
        "threshold = 0.6\n",
        "\n",
        "# test the model on a new image\n",
        "test_image = cv2.imread('test1.jpg')\n",
        "gray = cv2.cvtColor(test_image, cv2.COLOR_BGR2GRAY)\n",
        "faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=10)\n",
        "for (x,y,w,h) in faces:\n",
        "    face = test_image[y:y+h, x:x+w]\n",
        "    aligned_face = align_face(face)\n",
        "    if aligned_face is not None:\n",
        "        input_image = cv2.resize(aligned_face, (224, 224))\n",
        "        input_image = preprocess_input(input_image)\n",
        "        feature = model.predict(np.expand_dims(input_image, axis=0))[0]\n",
        "        index = match(feature, database, threshold)\n",
        "        if index is not None:\n",
        "            print(f\"Matched with image {index}\")\n",
        "        else:\n",
        "            print(\"No match found\")\n",
        "    else:\n",
        "        print(\"Face alignment failed\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
